{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Machine Learning Pipeline Workshop\n",
    "=============\n",
    "\n",
    "This notebook requires Spark 2.2+ to be pip installed your env.\n",
    "\n",
    "This workshop takes you through building a simple decision tree model using Spark's ML pipeline interface.\n",
    "\n",
    "The different algorithms options are contained in http://spark.apache.org/docs/latest/api/python/pyspark.ml.html\n",
    "The Spark ML documentation is at http://spark.apache.org/docs/latest/ml-pipeline.html \n",
    "\n",
    "If you're looking for Spark books why not consider Learning Spark or High Performance Spark (co-authored by the author of this notebook) -- http://amzn.to/2etDd0L\n",
    "\n",
    "Need more than the algorithms available in Spark? Or just have some time ot kill? Why not join us in https://github.com/sparklingpandas/sparklingml and add your own algorithm! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your SparkContext. You can set the master, by default this will use a local master unless you\n",
    "# add setMaster or change your enviorment variables.\n",
    "conf = SparkConf().setAppName(\"intro-to-ml\")\n",
    "# We use get or create here so that if the cell is evaluated multiple times we don't get multiple SparkContexts.\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start by downloading loading some data which is in csv format so its a good thing we got that csv package included already for us.\n",
    "\n",
    "Note: the data is a modified version of https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your running this on a cluster you will need to copy the data file into HDFS or whatever cluster file system\n",
    "# you are using\n",
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"resources/adult.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, workclass: string, fnlwgt: string, education: string, education-num: string, maritial-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: string, capital-loss: string, hours-per-week: string, native-country: string, category: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=u'39', workclass=u' State-gov', fnlwgt=u' 77516', education=u' Bachelors', education-num=u' 13', maritial-status=u' Never-married', occupation=u' Adm-clerical', relationship=u' Not-in-family', race=u' White', sex=u' Male', capital-gain=u' 2174', capital-loss=u' 0', hours-per-week=u' 40', native-country=u' United-States', category=u' <=50K')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see Spark has simply loaded all of the values as strings since we haven't specified another schema. We can isntead ask it to infer the schema and also handle this extra space magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"resources/adult.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=39, workclass=u' State-gov', fnlwgt=77516.0, education=u' Bachelors', education-num=13.0, maritial-status=u' Never-married', occupation=u' Adm-clerical', relationship=u' Not-in-family', race=u' White', sex=u' Male', capital-gain=2174.0, capital-loss=0.0, hours-per-week=40.0, native-country=u' United-States', category=u' <=50K')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, workclass: string, fnlwgt: double, education: string, education-num: double, maritial-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: double, capital-loss: double, hours-per-week: double, native-country: string, category: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has two different machine learning libraries. We will use the new Dataframe based one which is defined in pyspark.ml. Let's import the basics as well as DecisionTreeClassifier :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml.feature import Bucketizer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is prepairing the features, here we are just choosing existing numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"age\", \"education-num\"], outputCol=\"feautres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pipeline only works on doubles, so we need to take our category and turn it into a double. The StringIndexer will do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"category\").setOutputCol(\"category-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([assembler, indexer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to \"fit\" our pipeline. This allows the StringIndexer to determine what strings will be assigned what indexes in the eventual transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform our data into the prepaired format for our machine learning model to work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepared = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=39, workclass=u' State-gov', fnlwgt=77516.0, education=u' Bachelors', education-num=13.0, maritial-status=u' Never-married', occupation=u' Adm-clerical', relationship=u' Not-in-family', race=u' White', sex=u' Male', capital-gain=2174.0, capital-loss=0.0, hours-per-week=40.0, native-country=u' United-States', category=u' <=50K', feautres=DenseVector([39.0, 13.0]), category-index=0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol = \"category-index\", featuresCol=\"feautres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we fit on the prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4941aa8ed4a004643060) of depth 5 with 61 nodes"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But manually chaining this steps together makes it difficult to experiment. Instead we can put it together in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_and_model = Pipeline().setStages([assembler, indexer, dt])\n",
    "pipeline_model = pipeline_and_model.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=1.0, category-index=1.0),\n",
       " Row(prediction=1.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=1.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.transform(prepared).select(\"prediction\", \"category-index\").take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=1.0, category-index=1.0),\n",
       " Row(prediction=1.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=1.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=0.0, category-index=0.0),\n",
       " Row(prediction=1.0, category-index=1.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.transform(df).select(\"prediction\", \"category-index\").take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this shows us the results, it isn't super easy to read. Thankfully we can inverse these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(pipeline_model.stages[1].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "inverter = IndexToString(inputCol=\"prediction\", outputCol=\"prediction-label\", labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction-label=u' >50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' >50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' >50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' >50K'),\n",
       " Row(prediction-label=u' >50K', category=u' >50K'),\n",
       " Row(prediction-label=u' >50K', category=u' >50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' >50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' >50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' >50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' <=50K', category=u' <=50K'),\n",
       " Row(prediction-label=u' >50K', category=u' >50K')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverter.transform(pipeline_model.transform(df)).select(\"prediction-label\", \"category\").take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4941aa8ed4a004643060) of depth 5 with 61 nodes"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, min(hours-per-week): double, avg(hours-per-week): double, max(capital-gain): double]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.groupBy(\"age\").agg(min(\"hours-per-week\"), avg(\"hours-per-week\"), max(\"capital-gain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window.partitionBy(\"age\").orderBy(\"capital-gain\").rowsBetween(-100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|age|capital-gain|avg(capital-gain) OVER (PARTITION BY age ORDER BY capital-gain ASC NULLS FIRST ROWS BETWEEN 100 PRECEDING AND 100 FOLLOWING)|\n",
      "+---+------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "| 47|     99999.0|                                                                                                            16675.3786407767|\n",
      "| 44|     99999.0|                                                                                                           9770.615384615385|\n",
      "| 44|     99999.0|                                                                                                            9865.47572815534|\n",
      "| 44|     99999.0|                                                                                                           9962.196078431372|\n",
      "| 44|     99999.0|                                                                                                          10060.831683168317|\n",
      "| 28|     99999.0|                                                                                                           4736.762376237623|\n",
      "| 28|     99999.0|                                                                                                           4690.323529411765|\n",
      "| 22|     99999.0|                                                                                                           2644.735294117647|\n",
      "| 65|     99999.0|                                                                                                           6030.961538461538|\n",
      "| 22|     99999.0|                                                                                                           2670.920792079208|\n",
      "| 65|     99999.0|                                                                                                           6089.514563106796|\n",
      "| 53|     99999.0|                                                                                                           8464.336633663366|\n",
      "| 53|     99999.0|                                                                                                            8381.35294117647|\n",
      "| 31|     99999.0|                                                                                                           4793.683168316832|\n",
      "| 65|     99999.0|                                                                                                            6149.21568627451|\n",
      "| 53|     99999.0|                                                                                                           8220.173076923076|\n",
      "| 34|     99999.0|                                                                                                           5285.564356435643|\n",
      "| 65|     99999.0|                                                                                                          6210.0990099009905|\n",
      "| 78|     99999.0|                                                                                                           5208.217391304348|\n",
      "| 53|     99999.0|                                                                                                           8299.980582524271|\n",
      "+---+------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"age\"], df['capital-gain'], avg(\"capital-gain\").over(windowSpec)).orderBy(desc(\"capital-gain\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it's over to you to take it to the next steps!\n",
    "\n",
    "First:\n",
    "* Use Spark's built in CV model to see if DT is actually a good model type for this data\n",
    "\n",
    "After you've figured out how to measure effectiveness, it's time to see what we can do to improve:\n",
    "* What happens if we add additional features? Can we improve the DT?\n",
    "* What about different model types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
